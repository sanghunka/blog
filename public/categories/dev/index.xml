<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dev on Keep Moving</title>
    <link>https://sanghunka.github.io/categories/dev/</link>
    <description>Recent content in Dev on Keep Moving</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jan 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://sanghunka.github.io/categories/dev/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>zsh에 설정해둔 alias를 crontab에 사용하고 싶을때</title>
      <link>https://sanghunka.github.io/2018/01/zsh%EC%97%90-%EC%84%A4%EC%A0%95%ED%95%B4%EB%91%94-alias%EB%A5%BC-crontab%EC%97%90-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B3%A0-%EC%8B%B6%EC%9D%84%EB%95%8C/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2018/01/zsh%EC%97%90-%EC%84%A4%EC%A0%95%ED%95%B4%EB%91%94-alias%EB%A5%BC-crontab%EC%97%90-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B3%A0-%EC%8B%B6%EC%9D%84%EB%95%8C/</guid>
      <description>Put your functions in .zshenv.
.zshenv is sourced on all invocations of the shell, unless the -f option is set. It should contain commands to set the command search path, plus other important environment variables. .zshenv should not contain commands that produce output or assume the shell is attached to a tty.
.zshrc is sourced in interactive shells. It should contain commands to set up aliases, functions, options, key bindings, etc.</description>
    </item>
    
    <item>
      <title>[airflow] 6. Multi cluster에서 airflow 실행하기</title>
      <link>https://sanghunka.github.io/2017/12/airflow-6.-multi-cluster%EC%97%90%EC%84%9C-airflow-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0/</link>
      <pubDate>Wed, 20 Dec 2017 14:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/airflow-6.-multi-cluster%EC%97%90%EC%84%9C-airflow-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0/</guid>
      <description>요약  다루는 내용  분산 인스턴스에서 각각 airflow worker를 실행하고 task를 분산해서 실행하는법 task가 실행될 worker를 명시적으로 지정하는법  테스트 환경  두 개의 Amazon EC2 Instance 사용 1번 Instance에 아래와 같이 셋팅  metadata database(postsgres) rabbitmq airflow webserver airflow worker  2번 Instance에 아래와 같이 셋팅  airflow worker    airflow configuration  1번과 2번 instance에 airflow를 설치한다. dag폴더에 동일한 파일을 넣어준다. dag폴더를 Git repository로 세팅하고 Chef, Puppet, Ansible등으로 동기화 해주는 방법도 있다.</description>
    </item>
    
    <item>
      <title>[airflow] 5. Pyspark sample code on airflow</title>
      <link>https://sanghunka.github.io/2017/12/airflow-5.-pyspark-sample-code-on-airflow/</link>
      <pubDate>Wed, 20 Dec 2017 13:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/airflow-5.-pyspark-sample-code-on-airflow/</guid>
      <description>&lt;p&gt;Airflow에서 Pyspark task 실행하기&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[airflow] 5. Pyspark sample code on airflow</title>
      <link>https://sanghunka.github.io/2017/12/airflow-5.-pyspark-sample-code-on-airflow/</link>
      <pubDate>Wed, 20 Dec 2017 13:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/airflow-5.-pyspark-sample-code-on-airflow/</guid>
      <description>&lt;p&gt;Airflow에서 Pyspark task 실행하기&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Mac] ijavascript 설치</title>
      <link>https://sanghunka.github.io/2017/12/mac-ijavascript-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Wed, 20 Dec 2017 12:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/mac-ijavascript-%EC%84%A4%EC%B9%98/</guid>
      <description> ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; brew install pkg-config node zeromq sudo easy_install pip sudo pip install --upgrade pyzmq jupyter sudo npm install -g ijavascript   설치하고자 하는 가상환경에서 ijsinstall ijsnotebook으로 실행 </description>
    </item>
    
    <item>
      <title>[airflow] 4. CeleryExecutor 사용하기</title>
      <link>https://sanghunka.github.io/2017/12/airflow-4.-celeryexecutor-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</link>
      <pubDate>Tue, 05 Dec 2017 17:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/airflow-4.-celeryexecutor-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</guid>
      <description>&lt;p&gt;Airflow CeleryExecutor 사용하기&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[airflow] 3. LocalExecutor 사용하기</title>
      <link>https://sanghunka.github.io/2017/12/airflow-3.-localexecutor-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</link>
      <pubDate>Tue, 05 Dec 2017 16:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/airflow-3.-localexecutor-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</guid>
      <description>&lt;p&gt;Airflow LocalExecutor 사용하기&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Mac] Apche Spark 설치</title>
      <link>https://sanghunka.github.io/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/</guid>
      <description>Java 설치  이미 설치된 java의 경로를 찾고 싶다면 /usr/libexec/java_home 명령어를 이용하면 된다.
 brew tap caskroom/versions brew cask search java # brew cask install java 이렇게하면 자바9가 설치됩니다. brew cask install java8   2017-12-05 현재 Spark는 Java9를 지원하지 않는다. 그러므로 java8을 설치해야한다. 아래처럼 본인의 version에 맞는 path를 .bashrc(또는 .zshrc)에 지정해준다. export JAVA_HOME=&amp;quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&amp;quot;  Scala 설치 brew install scala   java와 마찬가지로 본인의 version에 맞는 path를 .bashrc(또는 .</description>
    </item>
    
    <item>
      <title>[Mac] pymssql 설치 에러날경우</title>
      <link>https://sanghunka.github.io/2017/12/mac-pymssql-%EC%84%A4%EC%B9%98-%EC%97%90%EB%9F%AC%EB%82%A0%EA%B2%BD%EC%9A%B0/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/mac-pymssql-%EC%84%A4%EC%B9%98-%EC%97%90%EB%9F%AC%EB%82%A0%EA%B2%BD%EC%9A%B0/</guid>
      <description> Mac에서 pymssql 설치시 에러가 나는경우 아래와 같은 방법을 시도해보자.  # 이미 freedts가 설치되어있다면 삭제해준다. # brew uninstall freedts brew install freetds091 brew link --force freetds@0.91 pip install pymssql </description>
    </item>
    
    <item>
      <title>MaxOS Hadoop 설치</title>
      <link>https://sanghunka.github.io/2017/12/maxos-hadoop-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/maxos-hadoop-%EC%84%A4%EC%B9%98/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;2.7.3기준으로 작성되어있음.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MaxOS Hive 설치</title>
      <link>https://sanghunka.github.io/2017/12/maxos-hive-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/maxos-hive-%EC%84%A4%EC%B9%98/</guid>
      <description>참조  https://cwiki.apache.org/confluence/display/Hive/GettingStarted https://noobergeek.wordpress.com/2013/11/09/simplest-way-to-install-and-configure-hive-for-mac-osx-lion/  1. Install hive via brew. This will take some time brew install hive  2. Add hadoop and hive to your path by editing your bash_profile  zshell사용자라면 .bash_profile대신 .zshrc vi ~/.bash_profile 아래 내용 추가 bash export HADOOP_HOME=/usr/local/Cellar/hadoop/hadoop.version.no export HIVE_HOME=/usr/local/Cellar/hive/hive.version.no/libexec  source ~/.bash_profile  3. Download the mysql connector  버전은 최신버전으로 다운받으면 좋음. ``` $ curl -L &amp;lsquo;http://www.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.22.tar.gz/from/http://mysql.he.net/&#39; | tar xz  $ sudo cp mysql-connector-java-5.</description>
    </item>
    
    <item>
      <title>MaxOS Hue 설치</title>
      <link>https://sanghunka.github.io/2017/12/maxos-hue-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/maxos-hue-%EC%84%A4%EC%B9%98/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;메뉴얼이 잘 되어있습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>[python]cat을 이용해서 stdin 인풋 주기</title>
      <link>https://sanghunka.github.io/2017/12/pythoncat%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%B4%EC%84%9C-stdin-%EC%9D%B8%ED%92%8B-%EC%A3%BC%EA%B8%B0/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/pythoncat%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%B4%EC%84%9C-stdin-%EC%9D%B8%ED%92%8B-%EC%A3%BC%EA%B8%B0/</guid>
      <description>&lt;p&gt;&lt;code&gt;cat iris.txt | python test.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[python]nbviewer 실행시키기</title>
      <link>https://sanghunka.github.io/2017/12/pythonnbviewer-%EC%8B%A4%ED%96%89%EC%8B%9C%ED%82%A4%EA%B8%B0/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/12/pythonnbviewer-%EC%8B%A4%ED%96%89%EC%8B%9C%ED%82%A4%EA%B8%B0/</guid>
      <description>&lt;p&gt;&lt;code&gt;cat iris.txt | python test.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[airflow] 2. 튜토리얼</title>
      <link>https://sanghunka.github.io/2017/11/airflow-2.-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/</link>
      <pubDate>Tue, 21 Nov 2017 17:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/airflow-2.-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/</guid>
      <description>&lt;p&gt;pipeline을 따라 만들어보며 Airflow의 concept, object, usage를 습득하기.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[airflow] 1. 설치</title>
      <link>https://sanghunka.github.io/2017/11/airflow-1.-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Tue, 21 Nov 2017 16:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/airflow-1.-%EC%84%A4%EC%B9%98/</guid>
      <description>&lt;p&gt;Airflow 설치하기&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[airflow] 0. Quickstart</title>
      <link>https://sanghunka.github.io/2017/11/airflow-0.-quickstart/</link>
      <pubDate>Tue, 21 Nov 2017 15:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/airflow-0.-quickstart/</guid>
      <description># airflow needs a home, ~/airflow is the default, # but you can lay foundation somewhere else if you prefer # (optional) export AIRFLOW_HOME=~/airflow # install from pypi using pip pip install airflow # initialize the database airflow initdb # start the web server, default port is 8080 airflow webserver -p 8080   export AIRFLOW_HOME=~/airflow 명령어로 설치 경로를 지정할 수 있다. AIRFLOW_HOME을 지정하지 않을 경우 default 경로는 ~/airflow 설치는 pip로 간단하게 할 수 있다.</description>
    </item>
    
    <item>
      <title>[Intro to Hadoop and MapReduce] Lesson 5 MapReduce Code</title>
      <link>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-5-mapreduce-code/</link>
      <pubDate>Mon, 06 Nov 2017 13:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-5-mapreduce-code/</guid>
      <description>1. Introduction 2. Quiz: Input Data How to find total sales/store?
 KEY VALUE  time store name cost store name store name cost store name product type  3. Quiz: Defensive Mapper Code # Your task is to make sure that this mapper code does not fail on corrupt data lines, # but instead just ignores them and continues working import sys def mapper(): # read standard input line by line for line in sys.</description>
    </item>
    
    <item>
      <title>[Intro to Hadoop and MapReduce] Lesson 4 Problem set</title>
      <link>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-4-problem-set/</link>
      <pubDate>Mon, 06 Nov 2017 12:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-4-problem-set/</guid>
      <description>1. Quiz: HDFS Which of the following is true?
HDFS uses a central SAN(storage area network) to hold its data HDFS stores a single copy of all data HDFS replicates all data for reliability To store 100TB of data in a Hadoop cluster you would need 300TB of raw disk space by default  2. Quiz: DataNode Which of the following is true if one of the nodes running the DataNode daemon on the cluster fails?</description>
    </item>
    
    <item>
      <title>[Intro to Hadoop and MapReduce] Lesson 3 HDFS and MapReduce</title>
      <link>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-3-hdfs-and-mapreduce/</link>
      <pubDate>Mon, 06 Nov 2017 11:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-3-hdfs-and-mapreduce/</guid>
      <description>1. Quiz: HDFS Is there a problem? &amp;gt; https://youtu.be/6F8-cCUbRU8
Network failure Disk failure on DN(datanode) Not all DN used Block sizes differ Disk failure on NN(namenode)  2. Quiz: Data Redundancy Any problem now?(when NN failure)
Data inaccessible &amp;gt; when network failure on NN Data lost forever &amp;gt; when disk failure on NN No problem  3. NameNode Standby The active namenode works before, but the standby can be configured to take over if the active one fails.</description>
    </item>
    
    <item>
      <title>[Intro to Hadoop and MapReduce] Lesson 2 Problem set</title>
      <link>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-2-problem-set/</link>
      <pubDate>Mon, 06 Nov 2017 10:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-2-problem-set/</guid>
      <description>1. Quiz: Dimensions of Big Data Which of the following are Part of the 3 dimensions of Big Data?
Volume Cost Importance Velocity Source Variety Security Virality  2. Quiz: Volume Volume of Big Data refers to:
Importance of Data Size of data Speed of data generation The differnet data sources  3. Quiz: Hadoop Ecosystem Check all that are true:
Hadoop provides an efficient way of storing data via HDFS Hadoop has a visualization framework called &amp;lsquo;Giraffe&amp;rsquo; You can analyze large datasets using a high-level language called &amp;lsquo;Pig&amp;rsquo; &amp;lsquo;Hive&amp;rsquo; offers a SQL-like language on top of MapReduce The tools in Hadoop&amp;rsquo;s ecosystem are all proprietary, commercial tools  4.</description>
    </item>
    
    <item>
      <title>[Intro to Hadoop and MapReduce] Lesson 1 Big data</title>
      <link>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-1-big-data/</link>
      <pubDate>Mon, 06 Nov 2017 09:00:00 +0900</pubDate>
      
      <guid>https://sanghunka.github.io/2017/11/intro-to-hadoop-and-mapreduce-lesson-1-big-data/</guid>
      <description>1. Introduction You can read more about Big Data in Wikipedia which is also a company that generates and processes huge amounts of data itself.
MapReduce and Apache Hadoop are the technologies we will be talking about more in this course.
2. Data Sources According to IBM: &amp;ldquo;Every day, 2.5 billion gigabytes of high-velocity data are created in a variety of forms, such as social media posts, information gathered in sensors and medical devices, videos and transaction records&amp;rdquo;</description>
    </item>
    
    <item>
      <title>github.io 블로그를 만들어 보자</title>
      <link>https://sanghunka.github.io/2017/05/github.io-%EB%B8%94%EB%A1%9C%EA%B7%B8%EB%A5%BC-%EB%A7%8C%EB%93%A4%EC%96%B4-%EB%B3%B4%EC%9E%90/</link>
      <pubDate>Tue, 09 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/05/github.io-%EB%B8%94%EB%A1%9C%EA%B7%B8%EB%A5%BC-%EB%A7%8C%EB%93%A4%EC%96%B4-%EB%B3%B4%EC%9E%90/</guid>
      <description>github page는 무엇인가? https://pages.github.com/ 는 깃허브에서 다이렉트로 호스팅해주는 서비스이다. 무료이다. 단 깃허브 계정 하나에 한 페이지만 만들 수 있다. 개발자들은 주로 포트폴리오나 cv, 또는 개인 블로그로 사용하고있다. 내 블로그인 sanghun.xyz도 깃허브 리포지터리를 이용한 페이지이다. 도메인은 aws에서 구매해 연결했다.
참조 나는 codecademy의 deploy-a-website를 따라해서 만들었다. 쉽고 친절한 수업이라 따라만 해도 쉽게 웹사이트를 배포할 수 있다. 물론 배포까지만! 그 다음은 개인의 웹개발 역량에 달려있다. 나는 괜찮아보이는 공개 theme를 가져다 썼다. 아래의 방법은 codecademy의 수업을 요약한 것이다.</description>
    </item>
    
    <item>
      <title>appsflyer pull api를 이용해 daily report 적재하기</title>
      <link>https://sanghunka.github.io/2017/02/appsflyer-pull-api%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-daily-report-%EC%A0%81%EC%9E%AC%ED%95%98%EA%B8%B0/</link>
      <pubDate>Thu, 09 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/02/appsflyer-pull-api%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-daily-report-%EC%A0%81%EC%9E%AC%ED%95%98%EA%B8%B0/</guid>
      <description>참조: appsflyer pull api 가이드
 push api가 아닌 pull api를 사용하는 이유.
 앱스플라이어 이벤트를 많이 정의할수록 push api가 빈번하게 호출된다. 이는 서비스 품질 저하로 이어질 수 있음. (완벽한 분석용 DB가 따로 구축되어 있다면 상관 없다.) 그러므로 실시간 데이터 적재보다는 하루에 한번, 전일자 데이터를 적재하기로 결정했고 이런 용도에는 pull api가 적합하다.  product와 archive에 동시에 적재하는 이유.
 앱스플라이어에서 데이터를 무한정 제공하진 않는다. 가입한 플랜에 따라 최근 x일자 데이터만 조회가 가능하다.</description>
    </item>
    
    <item>
      <title>appsflyer pull api를 이용해 raw data 적재하기</title>
      <link>https://sanghunka.github.io/2017/02/appsflyer-pull-api%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-raw-data-%EC%A0%81%EC%9E%AC%ED%95%98%EA%B8%B0/</link>
      <pubDate>Thu, 09 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/02/appsflyer-pull-api%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-raw-data-%EC%A0%81%EC%9E%AC%ED%95%98%EA%B8%B0/</guid>
      <description>참조: appsflyer pull api 가이드
 push api가 아닌 pull api를 사용하는 이유.
 앱스플라이어 이벤트를 많이 정의할수록 push api가 빈번하게 호출된다. 이는 서비스 품질 저하로 이어질 수 있음. (완벽한 분석용 DB가 따로 구축되어 있다면 상관 없다.) 그러므로 실시간 데이터 적재보다는 하루에 한번, 전일자 데이터를 적재하기로 결정했고 이런 용도에는 pull api가 적합하다.  product와 archive에 동시에 적재하는 이유.
 앱스플라이어에서 데이터를 무한정 제공하진 않는다. 가입한 플랜에 따라 최근 x일자 데이터만 조회가 가능하다.</description>
    </item>
    
    <item>
      <title>apache zeppelin에 postgres DB 연결하기</title>
      <link>https://sanghunka.github.io/2017/01/apache-zeppelin%EC%97%90-postgres-db-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0/</link>
      <pubDate>Mon, 23 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/01/apache-zeppelin%EC%97%90-postgres-db-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0/</guid>
      <description>
 제플린 빌드 후 오른쪽 상단에서 Interpreter 클릭  
 jdbc에서 DB 정보 입력
 common.max_count: 한번에 몇개의 row를 조회할 것인지 설정
 default.driver: org.postgresql.Driver
 default.password: DB 패스워드
 default.url: DB 주소. jdbc:postgresql://DNS_ADDRESS:PORT/DBNAME 형태로 입력.
 default.user: DB user name 입력
  
 notebook에서 첫줄에 %jdbc입력 후 테스트 쿼리를 날려보면 잘 되는걸 확인 할 수 있다. </description>
    </item>
    
    <item>
      <title>apache zeppelin의 dynamic form 정리</title>
      <link>https://sanghunka.github.io/2017/01/apache-zeppelin%EC%9D%98-dynamic-form-%EC%A0%95%EB%A6%AC/</link>
      <pubDate>Mon, 23 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2017/01/apache-zeppelin%EC%9D%98-dynamic-form-%EC%A0%95%EB%A6%AC/</guid>
      <description>&lt;p&gt;dynamic form을 이용하면 충분히 쓸만한 custom dashboard를 만들 수 있다.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>macOS Sierra에서 virtualenv에 Opencv3 설치</title>
      <link>https://sanghunka.github.io/2016/12/macos-sierra%EC%97%90%EC%84%9C-virtualenv%EC%97%90-opencv3-%EC%84%A4%EC%B9%98/</link>
      <pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2016/12/macos-sierra%EC%97%90%EC%84%9C-virtualenv%EC%97%90-opencv3-%EC%84%A4%EC%B9%98/</guid>
      <description>python3 설치  간단하게 brew로 설치하자  brew install python3  opencv3 설치  3은 아직 베타라고 한다. 안전한 버전을 원하면 opencv2를 설치하자. 나는 그냥 3설치 했다.  brew tap homebrew/science brew install opencv3 --with-python3 --with-ffmpeg --with-tbb --with-contrib  2016.12.04 임시 설치법  현재 mac OS Sierra에서 Opencv 설치에 문제가 있다. &amp;ndash;HEAD를 추가해 아래 방법대로 하면 된다. (16.12.4 기준) https://github.com/Homebrew/homebrew-science/issues/4104#issuecomment-249362870  brew install opencv3 --HEAD --with-python3 --with-ffmpeg --with-tbb --with-contrib  lookup 만들어주기  Ln -s {opencv의 site-packages} {사용하는 python환경의 site-packages} 형태로 lookup을 만들어 준다.</description>
    </item>
    
    <item>
      <title>jekyll github blog gemfile 버전 에러</title>
      <link>https://sanghunka.github.io/2016/11/jekyll-github-blog-gemfile-%EB%B2%84%EC%A0%84-%EC%97%90%EB%9F%AC/</link>
      <pubDate>Mon, 21 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2016/11/jekyll-github-blog-gemfile-%EB%B2%84%EC%A0%84-%EC%97%90%EB%9F%AC/</guid>
      <description>&lt;p&gt;이런 에러가 났다. config파일을 바꿔보고 루비와 jekyll을 지웠다 다시 깔아보고 했지만 모두 실패함.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>python에서 postgres DB 연결해서 쿼리 조회하기</title>
      <link>https://sanghunka.github.io/2016/11/python%EC%97%90%EC%84%9C-postgres-db-%EC%97%B0%EA%B2%B0%ED%95%B4%EC%84%9C-%EC%BF%BC%EB%A6%AC-%EC%A1%B0%ED%9A%8C%ED%95%98%EA%B8%B0/</link>
      <pubDate>Mon, 21 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sanghunka.github.io/2016/11/python%EC%97%90%EC%84%9C-postgres-db-%EC%97%B0%EA%B2%B0%ED%95%B4%EC%84%9C-%EC%BF%BC%EB%A6%AC-%EC%A1%B0%ED%9A%8C%ED%95%98%EA%B8%B0/</guid>
      <description>&lt;p&gt;2번 방법을 추천한다. 이유는 아래에서.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>